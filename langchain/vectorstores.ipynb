{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from clarifai.client.model import Model\n",
    "from clarifai.client.workflow import Workflow\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below is a complete C program that implements the backtracking algorithm to solve the N-Queens problem. The program places N queens on an N x N chessboard such that no two queens threaten each other.\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "\n",
      "#define N 8 // Change this value for a different size of the chessboard\n",
      "\n",
      "// Function to print the chessboard\n",
      "void printSolution(int board[N][N]) {\n",
      "    for (int i = 0; i < N; i++) {\n",
      "        for (int j = 0; j < N; j++) {\n",
      "            printf(\" %d \", board[i][j]);\n",
      "        }\n",
      "        printf(\"\\n\");\n",
      "    }\n",
      "}\n",
      "\n",
      "// Function to check if a queen can be placed at board[row][col]\n",
      "int isSafe(int board[N][N], int row, int col) {\n",
      "    // Check this row on left side\n",
      "    for (int i = 0; i < col; i++)\n",
      "        if (board[row][i])\n",
      "            return 0;\n",
      "\n",
      "    // Check upper diagonal on left side\n",
      "    for (int i = row, j = col; i >= 0 && j >= 0; i--, j--)\n",
      "        if (board[i][j])\n",
      "            return 0;\n",
      "\n",
      "    // Check lower diagonal on left side\n",
      "    for (int i = row, j = col; j >= 0 && i < N; i++, j--)\n",
      "        if (board[i][j])\n",
      "            return 0;\n",
      "\n",
      "    return 1;\n",
      "}\n",
      "\n",
      "// Backtracking function to solve N-Queens problem\n",
      "int solveNQUtil(int board[N][N], int col) {\n",
      "    // Base case: If all queens are placed\n",
      "    if (col >= N)\n",
      "        return 1;\n",
      "\n",
      "    // Consider this column and try placing this queen in all rows one by one\n",
      "    for (int i = 0; i < N; i++) {\n",
      "        if (isSafe(board, i, col)) {\n",
      "            // Place this queen\n",
      "            board[i][col] = 1;\n",
      "\n",
      "            // Recur to place the rest of the queens\n",
      "            if (solveNQUtil(board, col + 1))\n",
      "                return 1;\n",
      "\n",
      "            // If placing queen in board[i][col] doesn't lead to a solution\n",
      "            // then remove queen from board[i][col]\n",
      "            board[i][col] = 0; // BACKTRACK\n",
      "        }\n",
      "    }\n",
      "\n",
      "    // if the queen can not be placed in any row in this column col then return false\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "// Function to solve N-Queens problem using backtracking\n",
      "int solveNQ() {\n",
      "    int board[N][N];\n",
      "\n",
      "    // Initialize the chessboard with 0s\n",
      "    for (int i = 0; i < N; i++)\n",
      "        for (int j = 0; j < N; j++)\n",
      "            board[i][j] = 0;\n",
      "\n",
      "    if (solveNQUtil(board, 0) == 0) {\n",
      "        printf(\"Solution does not exist\");\n",
      "        return 0;\n",
      "    }\n",
      "\n",
      "    printSolution(board);\n",
      "    return 1;\n",
      "}\n",
      "\n",
      "// Main function\n",
      "int main() {\n",
      "    solveNQ();\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation of the Code:\n",
      "- The `N` macro defines the size of the chessboard (8x8 in this case).\n",
      "- The `printSolution` function displays the chessboard with queens placed.\n",
      "- The `isSafe` function checks if a queen can be safely placed at a given position without being attacked by other queens.\n",
      "- The `solveNQUtil` function implements the backtracking algorithm. It attempts to place queens column by column and backtracks if it finds a conflict.\n",
      "- The `solveNQ` function initializes the board and starts the backtracking process.\n",
      "- The `main` function calls the `solveNQ` function to start the program.\n",
      "\n",
      "### How to Compile and Run:\n",
      "1. Copy the code into a file named `n_queens.c`.\n",
      "2. Open a terminal and navigate to the directory containing the file.\n",
      "3. Compile the code using the command:\n",
      "   ```bash\n",
      "   gcc n_queens.c -o n_queens\n",
      "   ```\n",
      "4. Run the compiled program:\n",
      "   ```bash\n",
      "   ./n_queens\n",
      "   ```\n",
      "\n",
      "You can change the value of `N` to solve the N-Queens problem for different board sizes.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write me a code for backtracking of n queens in C language\\nWrite the entire code with main function not just the funcions used in backtracking\\nWrite a code which can be runned directly\"\n",
    "model_url=\"https://clarifai.com/openai/chat-completion/models/gpt-4o-mini\"\n",
    "\n",
    "\n",
    "# Model Predict\n",
    "model_prediction = Model(url=model_url,pat=os.getenv('CLARIFAI_PAT_TOKEN'),max_tokens=2000).predict_by_bytes(prompt.encode(), input_type=\"text\")\n",
    "\n",
    "print(model_prediction.outputs[0].data.text.raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt','w') as f:\n",
    "    f.write(model_prediction.outputs[0].data.text.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Workflow(user_id='clarifai', app_id='main', workflow_id='asr-sentiment',pat=os.getenv('CLARIFAI_PAT_TOKEN'))\n",
    "workflow_versions = workflow.list_versions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_url = \"https://s3.amazonaws.com/samples.clarifai.com/GreatPresentation2.wav\"\n",
    "\n",
    "prediction = workflow.predict_by_url(\n",
    "    audio_url,\n",
    "    input_type='audio'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concepts {\n",
      "  id: \"LABEL_2\"\n",
      "  name: \"LABEL_2\"\n",
      "  value: 0.980284452\n",
      "  app_id: \"text-classification\"\n",
      "}\n",
      "concepts {\n",
      "  id: \"LABEL_1\"\n",
      "  name: \"LABEL_1\"\n",
      "  value: 0.0179472286\n",
      "  app_id: \"text-classification\"\n",
      "}\n",
      "concepts {\n",
      "  id: \"LABEL_0\"\n",
      "  name: \"LABEL_0\"\n",
      "  value: 0.00176831812\n",
      "  app_id: \"text-classification\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prediction.results[0].outputs[-1].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['843bf252-b436-4b5c-bbd3-dcc05f506750',\n",
       " '8766f34b-13b6-482f-8bec-a9f24ef9d0ca',\n",
       " '4fe790db-fdb0-4fe2-bf83-aed254e2c595',\n",
       " '3e05b853-0fc9-4783-a2de-8ba9809222fd',\n",
       " '0da2e8fd-1114-42e9-a5f8-e5022342f0d6',\n",
       " '71117b4e-b8f6-4c47-8ee0-c72b58f6c56b',\n",
       " '7bec0d45-6db1-4abd-9dd1-cadfe197dc24',\n",
       " '3b0589b0-e1a1-45a1-8cc1-54c5a1ad85e2',\n",
       " '242ed814-a82c-4e3b-a7c5-4d780fee3cae',\n",
       " '8352acec-deba-4884-a61a-a9430a2be302']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_document_1 = Document(\n",
    "    page_content=\"I had chocalate chip pancakes and fried eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "updated_document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is sunny and warm, with a high of 82 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "vector_store.update_document(document_id=uuids[0], document=updated_document_1)\n",
    "# You can also update multiple documents at once\n",
    "vector_store.update_documents(\n",
    "    ids=uuids[:2], documents=[updated_document_1, updated_document_1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.541968] The stock market is down 500 points today due to fears of a recession. [{'source': 'news'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* I had chocalate chip pancakes and fried eggs for breakfast this morning. [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_by_vector(\n",
    "    embedding=embeddings.embed_query(\"I love green eggs and ham!\"), k=1\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASTRA_DB_API_ENDPOINT = os.getenv('ASTRA_DB_API_ENDPOINT')\n",
    "ASTRA_DB_APP_TOKEN = os.getenv('ASTRA_DB_APP_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = AstraDBVectorStore(\n",
    "    collection_name=\"astra_vector_langchain\",\n",
    "    embedding=embeddings,\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    token=ASTRA_DB_APP_TOKEN,\n",
    "    namespace=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['843bf252-b436-4b5c-bbd3-dcc05f506750',\n",
       " '8766f34b-13b6-482f-8bec-a9f24ef9d0ca',\n",
       " '4fe790db-fdb0-4fe2-bf83-aed254e2c595',\n",
       " '3e05b853-0fc9-4783-a2de-8ba9809222fd',\n",
       " '0da2e8fd-1114-42e9-a5f8-e5022342f0d6',\n",
       " '71117b4e-b8f6-4c47-8ee0-c72b58f6c56b',\n",
       " '7bec0d45-6db1-4abd-9dd1-cadfe197dc24',\n",
       " '3b0589b0-e1a1-45a1-8cc1-54c5a1ad85e2',\n",
       " '242ed814-a82c-4e3b-a7c5-4d780fee3cae',\n",
       " '8352acec-deba-4884-a61a-a9430a2be302']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.921394] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['843bf252-b436-4b5c-bbd3-dcc05f506750',\n",
       " '8766f34b-13b6-482f-8bec-a9f24ef9d0ca',\n",
       " '4fe790db-fdb0-4fe2-bf83-aed254e2c595',\n",
       " '3e05b853-0fc9-4783-a2de-8ba9809222fd',\n",
       " '0da2e8fd-1114-42e9-a5f8-e5022342f0d6',\n",
       " '71117b4e-b8f6-4c47-8ee0-c72b58f6c56b',\n",
       " '7bec0d45-6db1-4abd-9dd1-cadfe197dc24',\n",
       " '3b0589b0-e1a1-45a1-8cc1-54c5a1ad85e2',\n",
       " '242ed814-a82c-4e3b-a7c5-4d780fee3cae',\n",
       " '8352acec-deba-4884-a61a-a9430a2be302']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.314502] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aksha\\anaconda2\\envs\\nlp\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain-test-index1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['843bf252-b436-4b5c-bbd3-dcc05f506750',\n",
       " '8766f34b-13b6-482f-8bec-a9f24ef9d0ca',\n",
       " '4fe790db-fdb0-4fe2-bf83-aed254e2c595',\n",
       " '3e05b853-0fc9-4783-a2de-8ba9809222fd',\n",
       " '0da2e8fd-1114-42e9-a5f8-e5022342f0d6',\n",
       " '71117b4e-b8f6-4c47-8ee0-c72b58f6c56b',\n",
       " '7bec0d45-6db1-4abd-9dd1-cadfe197dc24',\n",
       " '3b0589b0-e1a1-45a1-8cc1-54c5a1ad85e2',\n",
       " '242ed814-a82c-4e3b-a7c5-4d780fee3cae',\n",
       " '8352acec-deba-4884-a61a-a9430a2be302']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.842749] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_api_key = os.getenv('QDRANT_API_KEY')\n",
    "url = os.getenv('QDRANT_CLUSTER_URL')\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    api_key=qdrant_api_key,\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
      "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\", k=2\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.841237] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    query=\"Will it be hot tomorrow\", k=1\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
