{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import Dict,List,Any\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "#     \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    # def on_llm_start(\n",
    "    #     self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    # ) -> None:\n",
    "    #     \"\"\"Run when chain starts running.\"\"\"\n",
    "    #     print(\"zzzz....\")\n",
    "    #     class_name = serialized[\"name\"]\n",
    "    #     print(\"Hi! I just woke up. Your llm is starting\")\n",
    "  \n",
    "\n",
    "    # def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "    #     \"\"\"Run when chain ends running.\"\"\"\n",
    "    #     print(\"zzzz....\")\n",
    "    #     print(\"Hi! I just woke up. Your llm is ending\")\n",
    "    \n",
    "class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "    async def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    api_key= os.getenv('GROQ_API_KEY'),\n",
    "    model='mixtral-8x7B-32768',\n",
    "    max_tokens=30,\n",
    "    callbacks=[MyCustomSyncHandler()]\n",
    ")\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model='gpt-3.5-turbo',\n",
    "#     api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "#     max_tokens=30\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"My custom handler, token: {token}\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"Tell me a joke about {animal}\"])\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in our custom handler as a list to the callbacks parameter\n",
    "\n",
    "chain = prompt | llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"animal\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why do bears have hairy coats?\\n\\nFur protection!' response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-c18f8909-cf93-4090-876d-4bd986e2e6f3-0' usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new PromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-01 20:43:22.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mcontent=\" The sum of 1 + 2 is 3. Is there anything specific you would like to know about this mathematical operation? I'm\" response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 16, 'total_tokens': 46, 'completion_time': 0.046979804, 'prompt_time': 0.002297612, 'queue_time': None, 'total_time': 0.049277416}, 'model_name': 'mixtral-8x7B-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'length', 'logprobs': None} id='run-5d649132-700b-49aa-a2b2-fe2107f9f2c3-0' usage_metadata={'input_tokens': 16, 'output_tokens': 30, 'total_tokens': 46}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import FileCallbackHandler, StdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from loguru import logger\n",
    "\n",
    "logfile = \"output.log\"\n",
    "\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler_1 = FileCallbackHandler(logfile)\n",
    "handler_2 = StdOutCallbackHandler()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# this chain will both print to stdout (because verbose=True) and write to 'output.log'\n",
    "# if verbose=False, the FileCallbackHandler will still write to 'output.log'\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"number\": 2}, {\"callbacks\":  [handler_1,handler_2]})\n",
    "logger.info(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
       "<html>\n",
       "<head>\n",
       "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
       "<title></title>\n",
       "<style type=\"text/css\">\n",
       ".ansi2html-content { display: inline; white-space: pre-wrap; word-wrap: break-word; }\n",
       ".body_foreground { color: #AAAAAA; }\n",
       ".body_background { background-color: #000000; }\n",
       ".inv_foreground { color: #000000; }\n",
       ".inv_background { background-color: #AAAAAA; }\n",
       ".ansi1 { font-weight: bold; }\n",
       ".ansi32 { color: #00aa00; }\n",
       ".ansi36 { color: #00aaaa; }\n",
       "</style>\n",
       "</head>\n",
       "<body class=\"body_foreground body_background\" style=\"font-size: normal;\" >\n",
       "<pre class=\"ansi2html-content\">\n",
       "\n",
       "\n",
       "<span class=\"ansi1\">&gt; Entering new RunnableSequence chain...</span>\n",
       "\n",
       "\n",
       "<span class=\"ansi1\">&gt; Entering new PromptTemplate chain...</span>\n",
       "\n",
       "<span class=\"ansi1\">&gt; Finished chain.</span>\n",
       "\n",
       "<span class=\"ansi1\">&gt; Finished chain.</span>\n",
       "<span class=\"ansi32\">2024-08-01 20:43:22.065</span> | <span class=\"ansi1\">INFO    </span> | <span class=\"ansi36\">__main__</span>:<span class=\"ansi36\">&lt;module&gt;</span>:<span class=\"ansi36\">18</span> - <span class=\"ansi1\">content=\" The sum of 1 + 2 is 3. Is there anything specific you would like to know about this mathematical operation? I'm\" response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 16, 'total_tokens': 46, 'completion_time': 0.046979804, 'prompt_time': 0.002297612, 'queue_time': None, 'total_time': 0.049277416}, 'model_name': 'mixtral-8x7B-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'length', 'logprobs': None} id='run-5d649132-700b-49aa-a2b2-fe2107f9f2c3-0' usage_metadata={'input_tokens': 16, 'output_tokens': 30, 'total_tokens': 46}</span>\n",
       "<span class=\"ansi32\">2024-08-01 20:43:22.065</span> | <span class=\"ansi1\">INFO    </span> | <span class=\"ansi36\">__main__</span>:<span class=\"ansi36\">&lt;module&gt;</span>:<span class=\"ansi36\">18</span> - <span class=\"ansi1\">content=\" The sum of 1 + 2 is 3. Is there anything specific you would like to know about this mathematical operation? I'm\" response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 16, 'total_tokens': 46, 'completion_time': 0.046979804, 'prompt_time': 0.002297612, 'queue_time': None, 'total_time': 0.049277416}, 'model_name': 'mixtral-8x7B-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'length', 'logprobs': None} id='run-5d649132-700b-49aa-a2b2-fe2107f9f2c3-0' usage_metadata={'input_tokens': 16, 'output_tokens': 30, 'total_tokens': 46}</span>\n",
       "<span class=\"ansi32\">2024-08-01 20:43:22.065</span> | <span class=\"ansi1\">INFO    </span> | <span class=\"ansi36\">__main__</span>:<span class=\"ansi36\">&lt;module&gt;</span>:<span class=\"ansi36\">18</span> - <span class=\"ansi1\">content=\" The sum of 1 + 2 is 3. Is there anything specific you would like to know about this mathematical operation? I'm\" response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 16, 'total_tokens': 46, 'completion_time': 0.046979804, 'prompt_time': 0.002297612, 'queue_time': None, 'total_time': 0.049277416}, 'model_name': 'mixtral-8x7B-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'length', 'logprobs': None} id='run-5d649132-700b-49aa-a2b2-fe2107f9f2c3-0' usage_metadata={'input_tokens': 16, 'output_tokens': 30, 'total_tokens': 46}</span>\n",
       "<span class=\"ansi32\">2024-08-01 20:43:22.065</span> | <span class=\"ansi1\">INFO    </span> | <span class=\"ansi36\">__main__</span>:<span class=\"ansi36\">&lt;module&gt;</span>:<span class=\"ansi36\">18</span> - <span class=\"ansi1\">content=\" The sum of 1 + 2 is 3. Is there anything specific you would like to know about this mathematical operation? I'm\" response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 16, 'total_tokens': 46, 'completion_time': 0.046979804, 'prompt_time': 0.002297612, 'queue_time': None, 'total_time': 0.049277416}, 'model_name': 'mixtral-8x7B-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'length', 'logprobs': None} id='run-5d649132-700b-49aa-a2b2-fe2107f9f2c3-0' usage_metadata={'input_tokens': 16, 'output_tokens': 30, 'total_tokens': 46}</span>\n",
       "\n",
       "</pre>\n",
       "</body>\n",
       "\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ansi2html import Ansi2HTMLConverter\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "with open(\"output.log\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "conv = Ansi2HTMLConverter()\n",
    "html = conv.convert(content, full=True)\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_chain_start AgentExecutor\n",
      "on_chain_start LLMChain\n",
      "on_llm_start ChatGoogleGenerativeAI\n",
      "on_agent_action tool='Calculator' tool_input='2**0.235' log='Thought: I need to calculate 2 raised to the power of 0.235.\\nAction: Calculator\\nAction Input: 2**0.235'\n",
      "on_tool_start Calculator\n",
      "on_chain_start LLMMathChain\n",
      "on_chain_start LLMChain\n",
      "on_llm_start ChatGoogleGenerativeAI\n",
      "on_chain_start LLMChain\n",
      "on_llm_start ChatGoogleGenerativeAI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1769067372187674'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.agents import AgentAction\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# First, define custom callback handler implementations\n",
    "class MyCustomHandlerOne(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start {serialized['name']}\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        print(f\"on_new_token {token}\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_start {serialized['name']}\")\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_tool_start {serialized['name']}\")\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        print(f\"on_agent_action {action}\")\n",
    "\n",
    "\n",
    "class MyCustomHandlerTwo(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start (I'm the second handler!!) {serialized['name']}\")\n",
    "\n",
    "\n",
    "# Instantiate the handlers\n",
    "handler1 = MyCustomHandlerOne()\n",
    "handler2 = MyCustomHandlerTwo()\n",
    "\n",
    "# Setup the agent. Only the `llm` will issue callbacks for handler2\n",
    "# llm = OpenAI(temperature=0, streaming=True, callbacks=[handler2])\n",
    "llm = ChatGoogleGenerativeAI(api_key=os.getenv('GOOGLE_API_KEY'),model='gemini-1.5-flash')\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
    "\n",
    "# Callbacks for handler1 will be issued by every object involved in the\n",
    "# Agent execution (llm, llmchain, tool, agent executor)\n",
    "agent.run(\"What is 2 raised to the 0.235 power?\", callbacks=[handler1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
